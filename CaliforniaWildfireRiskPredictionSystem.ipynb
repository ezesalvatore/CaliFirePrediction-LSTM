{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyOBi35tPxGCicR+Fx6sU2xm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ezesalvatore/CaliFirePrediction-LSTM/blob/main/CaliforniaWildfireRiskPredictionSystem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# California Wildfire Risk Prediction System\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "25LzU1YfcKTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Overview**\n",
        "  This machine learning project LSTM models with full-stack web development to create an wildfire prediction system for California.\n"
      ],
      "metadata": {
        "id": "VPqT-IDbCsDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Project Motivations**\n",
        "In recent years, California has experience massive wildfire seasons, with climate change only increasing the frequency and destructiveness of these fires. My mission is to prevent wildfires by predicting if they will happen. The prediction will give communities, firefighters, and other emergency services a head start."
      ],
      "metadata": {
        "id": "zX1j9IKJrTXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tech Stack**\n",
        "\n",
        "* **Datasets** of historical fire data\n",
        "* **LSTM models** built with TensorFlow/Keras trained in google collab\n",
        "* **Django** backends uses trained models with real-time API data\n",
        "* **React** displays interactive map interface with warnings"
      ],
      "metadata": {
        "id": "lp6Mk1n9rHtF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Documentation**\n",
        "\n",
        "This documents information about the California Wildfire Risk Prediction System dataset being used. This inculdes its source, date range, variables, limitations, and citation information.\n"
      ],
      "metadata": {
        "id": "YNAQhNW7rW2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CAL FIRE Historic Wildland Fire Perimeters Dataset Documentation\n",
        "**Date Range** 1878- Present (Oldest and Cohesive on Cal Fire documentation <br>\n",
        "**Data Source** [California Department of Forestry and Fire Protection's Fire and Resource Assessment Program (FRAP)](https://www.fire.ca.gov/what-we-do/fire-resource-assessment-program/fire-perimeters)\n",
        "###**Limiation of Dataset**\n",
        "This is the most complete dataset for califorina history, but there are 483 fires that are totally missing from the CAL FIRE Redbook \"Large, Damaging Fires\"\n",
        "###**Data Collection of dataset**\n",
        "CAL Fire document fires that are: <br> >= 10 acres in timber <br> >= 50 acres in brush <br> >= 300 acres in grass <br>  >= 1 fatality <br> >= 3 residential building burned\n",
        "###**Variables used:**\n",
        "`ALARM_DATE`= Fire start date <br>\n",
        "`geometry` = Fire perimeter coordinates <br>\n",
        "`GIS_ACRES` = Fire size in acres\n",
        "###**Variables from Code**\n",
        "`fire_occurred` = Binary target variable (0 = no fire, 1 = fire happened)<br>\n",
        "`fire_size_acres` = Filtered fire size (>= 10 acres only)<br>\n"
      ],
      "metadata": {
        "id": "77hZvU96Qc36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NOAA National Centers for Environmental Information Dataset Documentation**<br>\n",
        "**Date Range:** January 1, 2009 - December 31, 2013 <br>\n",
        "**Data Source:** [NOAA GHCN-Daily dataset](https://www.ncei.noaa.gov/cdo-web/search?datasetid=GHCND)\n",
        "###**Station Location:**\n",
        "Arcata Eureka Airport `USW00024283` <br>\n",
        "Redding Airport `USW00024257` <br> Marysville Airport `USW00093205`<br>\n",
        "Napa Airport `USW00093227`  <br> Santa Maria Airport `USW00023273`  <br> Watsonville Airport `USW00023277` <br> Stockton Airport `USW00023237` <br>\n",
        "Merced Municipal Airport `USW00023257`<br> Fresno Yosemite International `USW00093193` <br> Bakersfield Airport `USW00023155`  <br>\n",
        " Santa Barbara 11W `USW00023190`<br>  Burbank-Glendale-Pasadena Airport `USW00023152` <br> Oceanside Airport `USW00053121`\n",
        "\n",
        "###**Variables Collected from Dataset:**\n",
        "`PRCP` = Precipitation (mm) <br>\n",
        "`TMAX`= Max Temp (Â°C) <br>\n",
        "`TMIN` = Min Temp (Â°C) <br>\n",
        "`AWND` = Average Wind Speed (m/s)\n",
        "\n",
        "###**Variables from Code**\n",
        "`temperature_range` = TMAX-TMIN <br>\n",
        "`days_since_rain` = Days with PRCP < 0.25 <br>\n",
        "`fire_season` = May - October period <br>\n",
        "`station_id` = Weather station <br>\n",
        "\n",
        "\n",
        "###**Citation**\n",
        "National Centers for Environmental Information, NOAA.(2025)\n"
      ],
      "metadata": {
        "id": "bneZLYKtH_8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Modis NDVI Dataset**\n",
        "**Date Range:** 2000-Present <br>\n",
        "**Data Source:** [Google Earth Engine - MODIS/061/MOD13Q1](https://developers.google.com/earth-engine/datasets/catalog/MODIS_061_MOD13Q1)\n",
        "\n",
        "###**Regional Coverage**\n",
        "Since Califorina is made up with different regions with vastly differnt climate I separated the region my polygons will collect the data\n",
        "\n",
        "###**Data Collection Method**\n",
        "```python\n",
        "modis_ndvi = ee.ImageCollection('MODIS/061/MOD13Q1') \\\n",
        "    .select('NDVI') \\\n",
        "    .filterDate('2009-01-01', '2013-12-31')\n",
        "```\n",
        "\n",
        "###**Variables from Dataset**\n",
        "`NDVI`= Raw MODIS NDVI band values (range: -1 to +1)\n",
        "\n",
        "###**Variables from Code**\n",
        "`ndvi_mean` = average NDVI for each fire region <br>\n",
        "`ndvi_stdev` = measure of NDVI variation<br>\n",
        "`region` = fire zone\n",
        "\n",
        "###**Citation**\n",
        "LP DAAC. (2021). MOD13Q1 MODIS/Terra Vegetation Indices 16-Day L3 Global 250m V061. NASA EOSDIS Land Processes DAAC."
      ],
      "metadata": {
        "id": "J2nwORqrNmZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Dataset Creation**"
      ],
      "metadata": {
        "id": "Fgs4BJeKqoBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base_path = '/content/drive/MyDrive/fire_prediction_project'\n",
        "folders = [\n",
        "    'raw_data/fires_calfire',\n",
        "    'raw_data/weather_ghcn',\n",
        "    'raw_data/vegetation_modis',\n",
        "]\n",
        "\n",
        "for folder in folders:\n",
        "    full_path = f'{base_path}/{folder}'\n",
        "    os.makedirs(full_path, exist_ok=True)\n",
        "    print(f\"Created: {folder}\")"
      ],
      "metadata": {
        "id": "AM4HmM5yq5Ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if data exists\n",
        "\n",
        "print(os.listdir('/content/drive/MyDrive/fire_prediction_project/raw_data/fires_calfire'))\n",
        "print(os.listdir('/content/drive/MyDrive/fire_prediction_project/raw_data/weather_ghcn'))\n",
        "print(os.listdir('/content/drive/MyDrive/fire_prediction_project/raw_data/vegetation_modis'))\n"
      ],
      "metadata": {
        "id": "iWd5cZ7et4w0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fire Perimeters Loading, Cleaning, Visulization data retrieved**"
      ],
      "metadata": {
        "id": "rEJzgBdEecnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "\n",
        "def load_fire_data():\n",
        "    \"\"\"\n",
        "    Loads CAL FIRE data and creates LSTM targets directly.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two pandas DataFrames:\n",
        "            - study_fires (pd.DataFrame or None): DataFrame of filtered fire incidents within the study period, or None if loading fails.\n",
        "            - daily_targets (pd.DataFrame or None): DataFrame of daily fire occurrence targets for LSTM, or None if loading fails.\n",
        "    \"\"\"\n",
        "    fire_folder = '/content/drive/MyDrive/fire_prediction_project/raw_data/fires_calfire'\n",
        "    print(f\"Looking for fire data in: {fire_folder}\")\n",
        "\n",
        "    if not os.path.exists(fire_folder):\n",
        "        print(\"Error: Fire folder not found!\")\n",
        "        return None, None\n",
        "\n",
        "    shp_files = [f for f in os.listdir(fire_folder) if f.endswith('.shp')]\n",
        "    if not shp_files:\n",
        "        print(\"Note: No fire shapefiles found.\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"Found {len(shp_files)} shapefile(s)\")\n",
        "    first_shp_file = f'{fire_folder}/{shp_files[0]}'\n",
        "\n",
        "    try:\n",
        "        fire_gdf = gpd.read_file(first_shp_file)\n",
        "        print(f\"Loaded {len(fire_gdf)} fire records\")\n",
        "        return process_fire_data(fire_gdf)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading shapefile: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def process_fire_data(fire_gdf):\n",
        "    \"\"\"\n",
        "    Process fire data and create LSTM targets.\n",
        "\n",
        "    Args:\n",
        "        fire_gdf (gpd.GeoDataFrame): GeoDataFrame containing the raw fire data.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two pandas DataFrames:\n",
        "            - study_fires (pd.DataFrame): DataFrame of processed and filtered fire incidents.\n",
        "            - daily_targets (pd.DataFrame): DataFrame of daily fire occurrence targets for LSTM.\n",
        "    \"\"\"\n",
        "    print(\"Processing fire data...\")\n",
        "\n",
        "    # Convert to World Geodetic System 1984 which is a common lat/longitude system\n",
        "    if fire_gdf.crs != 'EPSG:4326':\n",
        "        fire_gdf = fire_gdf.to_crs('EPSG:4326')\n",
        "\n",
        "    # Drop the geometry to get the center of the fire perimeter\n",
        "    fire_df = pd.DataFrame(fire_gdf.drop(columns='geometry'))\n",
        "\n",
        "    # Get the center of fire geometry\n",
        "    centroids = fire_gdf.geometry.centroid\n",
        "    fire_df['centroid_lat'] = centroids.y\n",
        "    fire_df['centroid_lon'] = centroids.x\n",
        "\n",
        "    # Updated comprehensive weather station coordinates\n",
        "    weather_stations = [\n",
        "        {'name': 'Arcata Eureka Airport', 'usw_id': 'USW00024283', 'lat': 40.97844, 'lon': -124.10479},\n",
        "        {'name': 'Redding Airport', 'usw_id': 'USW00024257', 'lat': 40.51, 'lon': -122.29},\n",
        "        {'name': 'Marysville Airport (Beale AFB)', 'usw_id': 'USW00093205', 'lat': 39.136089, 'lon': -121.436567},\n",
        "        {'name': 'Napa Airport', 'usw_id': 'USW00093227', 'lat': 38.213194, 'lon': -122.280694},\n",
        "        {'name': 'Stockton Airport', 'usw_id': 'USW00023237', 'lat': 37.88997, 'lon': -121.22637},\n",
        "        {'name': 'Fresno Yosemite International', 'usw_id': 'USW00093193', 'lat': 36.77999, 'lon': -119.72016},\n",
        "        {'name': 'Santa Maria Airport', 'usw_id': 'USW00023273', 'lat': 34.8927, 'lon': -120.4545},\n",
        "        {'name': 'Watsonville Airport', 'usw_id': 'USW00023277', 'lat': 36.935, 'lon': -121.79},\n",
        "        {'name': 'Merced Municipal Airport', 'usw_id': 'USW00023257', 'lat': 37.28470, 'lon': -120.51400},\n",
        "        {'name': 'Bakersfield Airport', 'usw_id': 'USW00023155', 'lat': 35.3217, 'lon': -118.9910},\n",
        "        {'name': 'Santa Barbara 11W', 'usw_id': 'USW00023190', 'lat': 34.4208, 'lon': -119.6982},\n",
        "        {'name': 'Burbank-Glendale-Pasadena Airport', 'usw_id': 'USW00023152', 'lat': 34.2003, 'lon': -118.3552},\n",
        "        {'name': 'Oceanside Airport', 'usw_id': 'USW00023181', 'lat': 33.218, 'lon': -117.351}\n",
        "    ]\n",
        "\n",
        "    def find_nearest_weather_station(lat, lon):\n",
        "        \"\"\"\n",
        "        Find the nearest weather station to given coordinates.\n",
        "\n",
        "        Args:\n",
        "            lat (float): Latitude coordinate of the fire\n",
        "            lon (float): Longitude coordinate of the fire\n",
        "\n",
        "        Returns:\n",
        "            str: USW ID of the nearest weather station\n",
        "        \"\"\"\n",
        "        if pd.isna(lat) or pd.isna(lon):\n",
        "            return 'Unknown'\n",
        "\n",
        "        # Check if within California bounds (rough bounds)\n",
        "        if not (32.4 <= lat <= 42.2 and -124.6 <= lon <= -114.0):\n",
        "            return 'Unknown'\n",
        "\n",
        "        min_distance = float('inf')\n",
        "        nearest_station = 'Unknown'\n",
        "\n",
        "        for station in weather_stations:\n",
        "            # Calculate Euclidean distance (approximate since we're dealing with small distances)\n",
        "            distance = np.sqrt((lat - station['lat'])**2 + (lon - station['lon'])**2)\n",
        "\n",
        "            if distance < min_distance:\n",
        "                min_distance = distance\n",
        "                nearest_station = station['usw_id']\n",
        "\n",
        "        return nearest_station\n",
        "\n",
        "    # Assign each fire to its nearest weather station\n",
        "    fire_df['weather_station'] = fire_df.apply(\n",
        "        lambda row: find_nearest_weather_station(row['centroid_lat'], row['centroid_lon']),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Renaming columns to match expected format\n",
        "    column_mapping = {\n",
        "        'ALARM_DATE': 'fire_date',\n",
        "        'GIS_ACRES': 'fire_size_acres',\n",
        "        'YEAR_': 'year'\n",
        "    }\n",
        "    for old_name, new_name in column_mapping.items():\n",
        "        if old_name in fire_df.columns:\n",
        "            fire_df = fire_df.rename(columns={old_name: new_name})\n",
        "\n",
        "    # Process dates and filter\n",
        "    if 'fire_date' not in fire_df.columns:\n",
        "        print(\"Warning: 'fire_date' column not found.\")\n",
        "        return None, None\n",
        "\n",
        "    fire_df['fire_date'] = pd.to_datetime(fire_df['fire_date'], errors='coerce')\n",
        "    fire_df = fire_df.dropna(subset=['fire_date'])\n",
        "\n",
        "    # Filter to study period and significant fires\n",
        "    study_fires = fire_df[\n",
        "        (fire_df['fire_date'] >= '2009-01-01') &\n",
        "        (fire_df['fire_date'] <= '2013-12-31')\n",
        "    ].copy()\n",
        "\n",
        "    if 'fire_size_acres' in study_fires.columns:\n",
        "        study_fires = study_fires[study_fires['fire_size_acres'] >= 10]\n",
        "\n",
        "    print(f\"Study period fires: {len(study_fires)} (2009-2013)\")\n",
        "\n",
        "    # Print distribution by weather station\n",
        "    print(\"\\nFire distribution by weather station:\")\n",
        "    station_counts = study_fires['weather_station'].value_counts()\n",
        "    for station_id, count in station_counts.items():\n",
        "        station_name = next((s['name'] for s in weather_stations if s['usw_id'] == station_id), station_id)\n",
        "        print(f\"  {station_name} ({station_id}): {count} fires\")\n",
        "\n",
        "    # Create LSTM targets\n",
        "    daily_targets = create_daily_fire_targets(study_fires)\n",
        "\n",
        "    return study_fires, daily_targets\n",
        "\n",
        "def create_daily_fire_targets(fire_df):\n",
        "    \"\"\"\n",
        "    Creates 5-day fire prediction targets for LSTM training.\n",
        "\n",
        "    Args:\n",
        "        fire_df (pd.DataFrame): DataFrame of fire incidents with 'fire_date' and 'weather_station' columns.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with daily 5-day fire prediction targets.\n",
        "    \"\"\"\n",
        "    print(\"Creating 5-day fire prediction targets...\")\n",
        "\n",
        "    if 'fire_date' not in fire_df.columns or 'weather_station' not in fire_df.columns:\n",
        "        print(\"Warning: Missing required columns for targets.\")\n",
        "        return None\n",
        "\n",
        "    # Create date range - end 5 days early to allow for 5-day prediction window\n",
        "    date_range = pd.date_range('2009-01-01', '2013-12-26', freq='D')  # Dec 26 instead of Dec 31\n",
        "    stations = fire_df['weather_station'].unique()\n",
        "    stations = [s for s in stations if s != 'Unknown']\n",
        "\n",
        "    # Build daily targets\n",
        "    daily_targets = []\n",
        "    for date in date_range:\n",
        "        for station in stations:\n",
        "            # Look for fires in the next 5 days (tomorrow through day 5)\n",
        "            window_start = date + pd.Timedelta(days=1)\n",
        "            window_end = date + pd.Timedelta(days=5)\n",
        "\n",
        "            fires_in_window = fire_df[\n",
        "                (fire_df['fire_date'] >= window_start) &\n",
        "                (fire_df['fire_date'] <= window_end) &\n",
        "                (fire_df['weather_station'] == station)\n",
        "            ]\n",
        "\n",
        "            daily_targets.append({\n",
        "                'date': date,\n",
        "                'weather_station': station,\n",
        "                'fire_within_5_days': 1 if len(fires_in_window) > 0 else 0\n",
        "            })\n",
        "\n",
        "    targets_df = pd.DataFrame(daily_targets)\n",
        "\n",
        "    # Print statistics\n",
        "    positive_rate = targets_df['fire_within_5_days'].mean() * 100\n",
        "    print(f\"Created {len(targets_df)} daily target records\")\n",
        "    print(f\"5-day fire prediction rate: {positive_rate:.1f}%\")\n",
        "\n",
        "    return targets_df\n",
        "\n",
        "# Load and process fire data\n",
        "print(\"Loading fire data with updated weather stations...\")\n",
        "fire_data, fire_targets = load_fire_data()\n",
        "\n",
        "print(f\"\\nFire data shape: {fire_data.shape if fire_data is not None else 'None'}\")\n",
        "print(f\"LSTM targets shape: {fire_targets.shape if fire_targets is not None else 'None'}\")"
      ],
      "metadata": {
        "id": "cLzqSazDekyD",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "def visualize_fire_data(fire_data, fire_targets):\n",
        "    \"\"\"Simple fire data visualizations\"\"\"\n",
        "\n",
        "    # Set up the plot style\n",
        "    plt.style.use('default')\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('ðŸ”¥ California Fire Data Overview (2009-2013)', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # 1. Fires by Weather Station\n",
        "    station_counts = fire_data['weather_station'].value_counts()\n",
        "    axes[0,0].bar(range(len(station_counts)), station_counts.values, color='orangered', alpha=0.7)\n",
        "    axes[0,0].set_title('Total Fires by Region')\n",
        "    axes[0,0].set_ylabel('Number of Fires')\n",
        "    axes[0,0].set_xlabel('Weather Stations')\n",
        "    axes[0,0].set_xticks(range(len(station_counts)))\n",
        "    axes[0,0].set_xticklabels([s[-6:] for s in station_counts.index], rotation=45)  # Show last 6 chars of station ID\n",
        "\n",
        "    # 2. Fire Size Distribution\n",
        "    axes[0,1].hist(fire_data['fire_size_acres'], bins=30, color='red', alpha=0.6, edgecolor='black')\n",
        "    axes[0,1].set_title('Fire Size Distribution')\n",
        "    axes[0,1].set_xlabel('Fire Size (acres)')\n",
        "    axes[0,1].set_ylabel('Number of Fires')\n",
        "    axes[0,1].set_yscale('log')  # Log scale because of wide range\n",
        "\n",
        "    # 3. Monthly Fire Pattern\n",
        "    fire_data['month'] = fire_data['fire_date'].dt.month\n",
        "    monthly_fires = fire_data['month'].value_counts().sort_index()\n",
        "    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
        "              'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "\n",
        "    axes[1,0].plot(monthly_fires.index, monthly_fires.values, 'o-',\n",
        "                   linewidth=3, markersize=8, color='darkred')\n",
        "    axes[1,0].set_title('Fires by Month')\n",
        "    axes[1,0].set_xlabel('Month')\n",
        "    axes[1,0].set_ylabel('Number of Fires')\n",
        "    axes[1,0].set_xticks(range(1, 13))\n",
        "    axes[1,0].set_xticklabels(months, rotation=45)\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. Fire Risk Rate by Station\n",
        "    if 'fire_within_5_days' in fire_targets.columns:\n",
        "        risk_by_station = fire_targets.groupby('weather_station')['fire_within_5_days'].mean() * 100\n",
        "    else:\n",
        "        # Fallback if column name is different\n",
        "        risk_by_station = fire_targets.groupby('weather_station').iloc[:,2].mean() * 100\n",
        "\n",
        "    axes[1,1].bar(range(len(risk_by_station)), risk_by_station.values,\n",
        "                  color='orange', alpha=0.7)\n",
        "    axes[1,1].set_title('5-Day Fire Risk by Region')\n",
        "    axes[1,1].set_ylabel('Fire Risk (%)')\n",
        "    axes[1,1].set_xlabel('Weather Stations')\n",
        "    axes[1,1].set_xticks(range(len(risk_by_station)))\n",
        "    axes[1,1].set_xticklabels([s[-6:] for s in risk_by_station.index], rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print Summary Statistics\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ðŸ”¥ FIRE DATA SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"ðŸ“Š Total Fires: {len(fire_data):,}\")\n",
        "    print(f\"ðŸ“… Date Range: {fire_data['fire_date'].min().strftime('%Y-%m-%d')} to {fire_data['fire_date'].max().strftime('%Y-%m-%d')}\")\n",
        "    print(f\"ðŸŒ¡ï¸ Weather Stations: {fire_data['weather_station'].nunique()}\")\n",
        "    print(f\"ðŸ”¢ Average Fire Size: {fire_data['fire_size_acres'].mean():.0f} acres\")\n",
        "    print(f\"ðŸ“ˆ Largest Fire: {fire_data['fire_size_acres'].max():,.0f} acres\")\n",
        "\n",
        "    # Fire season analysis\n",
        "    fire_season_fires = fire_data[fire_data['fire_date'].dt.month.isin([5,6,7,8,9,10])]\n",
        "    fire_season_pct = (len(fire_season_fires) / len(fire_data)) * 100\n",
        "    print(f\"ðŸŒ¡ï¸ Fire Season (May-Oct): {fire_season_pct:.1f}% of all fires\")\n",
        "\n",
        "    # LSTM targets summary\n",
        "    if fire_targets is not None:\n",
        "        total_days = len(fire_targets)\n",
        "        fire_days = fire_targets.iloc[:,2].sum() if fire_targets.shape[1] > 2 else 0\n",
        "        overall_risk = (fire_days / total_days) * 100 if total_days > 0 else 0\n",
        "        print(f\"ðŸŽ¯ Overall 5-Day Fire Risk: {overall_risk:.1f}%\")\n",
        "        print(f\"ðŸ“‹ LSTM Training Records: {total_days:,}\")\n",
        "\n",
        "    print(\"=\"*50)\n",
        "\n",
        "# Simple usage example:\n",
        "visualize_fire_data(fire_data, fire_targets)"
      ],
      "metadata": {
        "id": "7DsU1YzXBfVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NOAA load, clean, visulization, and retrival**"
      ],
      "metadata": {
        "id": "SfzFnwjFVeBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def load_weather_data():\n",
        "    \"\"\"\n",
        "    Load and process weather data from NOAA GHCN-Daily dataset for fire prediction.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Processed weather data with engineered features, or None if loading fails.\n",
        "    \"\"\"\n",
        "    weather_path = '/content/drive/MyDrive/fire_prediction_project/raw_data/weather_ghcn/4030855.csv'\n",
        "\n",
        "    print(f\"Loading weather data from: {weather_path}\")\n",
        "\n",
        "    if not os.path.exists(weather_path):\n",
        "        print(\"Error: Weather file not found!\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Load the weather data\n",
        "        weather_df = pd.read_csv(weather_path)\n",
        "        print(f\"Loaded {len(weather_df)} weather records\")\n",
        "\n",
        "        # Process the weather data\n",
        "        processed_weather = process_weather_data(weather_df)\n",
        "\n",
        "        return processed_weather\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading weather data: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_weather_data(weather_df):\n",
        "    \"\"\"\n",
        "    Process raw weather data and create engineered features.\n",
        "\n",
        "    Args:\n",
        "        weather_df (pd.DataFrame): Raw weather data from NOAA\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Processed weather data with engineered features\n",
        "    \"\"\"\n",
        "    print(\"Processing weather data...\")\n",
        "\n",
        "    # Convert date column\n",
        "    weather_df['DATE'] = pd.to_datetime(weather_df['DATE'])\n",
        "\n",
        "    # Filter to study period\n",
        "    weather_df = weather_df[\n",
        "        (weather_df['DATE'] >= '2009-01-01') &\n",
        "        (weather_df['DATE'] <= '2013-12-31')\n",
        "    ].copy()\n",
        "\n",
        "    # Rename columns for consistency\n",
        "    weather_df = weather_df.rename(columns={\n",
        "        'DATE': 'date',\n",
        "        'STATION': 'weather_station'\n",
        "    })\n",
        "\n",
        "    # Convert temperature from tenths of degrees C to degrees C\n",
        "    if 'TMAX' in weather_df.columns:\n",
        "        weather_df['TMAX'] = weather_df['TMAX'] / 10.0\n",
        "    if 'TMIN' in weather_df.columns:\n",
        "        weather_df['TMIN'] = weather_df['TMIN'] / 10.0\n",
        "\n",
        "    # Convert precipitation from tenths of mm to mm\n",
        "    if 'PRCP' in weather_df.columns:\n",
        "        weather_df['PRCP'] = weather_df['PRCP'] / 10.0\n",
        "\n",
        "    # Convert wind speed from tenths of m/s to m/s\n",
        "    if 'AWND' in weather_df.columns:\n",
        "        weather_df['AWND'] = weather_df['AWND'] / 10.0\n",
        "\n",
        "    # Create engineered features\n",
        "    weather_df = create_weather_features(weather_df)\n",
        "\n",
        "    print(f\"Processed weather data: {len(weather_df)} records\")\n",
        "    print(f\"Date range: {weather_df['date'].min().date()} to {weather_df['date'].max().date()}\")\n",
        "    print(f\"Weather stations: {weather_df['weather_station'].nunique()}\")\n",
        "\n",
        "    return weather_df\n",
        "\n",
        "def create_weather_features(weather_df):\n",
        "    \"\"\"\n",
        "    Create engineered weather features for fire prediction.\n",
        "\n",
        "    Args:\n",
        "        weather_df (pd.DataFrame): Basic weather data\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Weather data with engineered features\n",
        "    \"\"\"\n",
        "    print(\"Creating weather features...\")\n",
        "\n",
        "    # Temperature range\n",
        "    if 'TMAX' in weather_df.columns and 'TMIN' in weather_df.columns:\n",
        "        weather_df['temperature_range'] = weather_df['TMAX'] - weather_df['TMIN']\n",
        "\n",
        "    # Fire season indicator (May - October)\n",
        "    weather_df['fire_season'] = weather_df['date'].dt.month.isin([5, 6, 7, 8, 9, 10]).astype(int)\n",
        "\n",
        "    # Add wind strength features\n",
        "    weather_df = add_wind_strength(weather_df)\n",
        "\n",
        "    # Days since rain (calculated per weather station)\n",
        "    weather_df = weather_df.sort_values(['weather_station', 'date'])\n",
        "\n",
        "    def calculate_days_since_rain(group):\n",
        "        \"\"\"Calculate days since last significant rain (>= 0.25mm) for each station\"\"\"\n",
        "        group = group.copy()\n",
        "        group['days_since_rain'] = 0\n",
        "\n",
        "        if 'PRCP' in group.columns:\n",
        "            last_rain_day = 0\n",
        "            for i, row in group.iterrows():\n",
        "                if pd.notna(row['PRCP']) and row['PRCP'] >= 0.25:\n",
        "                    last_rain_day = 0\n",
        "                else:\n",
        "                    last_rain_day += 1\n",
        "                group.loc[i, 'days_since_rain'] = last_rain_day\n",
        "\n",
        "        return group\n",
        "\n",
        "    # Apply days since rain calculation by weather station\n",
        "    weather_df = weather_df.groupby('weather_station').apply(calculate_days_since_rain).reset_index(drop=True)\n",
        "\n",
        "    # Fill missing values\n",
        "    weather_df['PRCP'] = weather_df['PRCP'].fillna(0)\n",
        "    weather_df['AWND'] = weather_df['AWND'].fillna(weather_df['AWND'].mean())\n",
        "\n",
        "    # Select final columns\n",
        "    feature_columns = [\n",
        "        'date', 'weather_station', 'PRCP', 'TMAX', 'TMIN', 'AWND',\n",
        "        'temperature_range', 'days_since_rain', 'fire_season',\n",
        "        'wind_mph', 'wind_strength'\n",
        "    ]\n",
        "\n",
        "    # Only keep columns that exist\n",
        "    available_columns = [col for col in feature_columns if col in weather_df.columns]\n",
        "    weather_df = weather_df[available_columns]\n",
        "\n",
        "    print(f\"Weather features created: {list(weather_df.columns)}\")\n",
        "\n",
        "    # Print basic statistics\n",
        "    print(\"\\nWeather data summary:\")\n",
        "    print(f\"  Average TMAX: {weather_df['TMAX'].mean():.1f}Â°C\")\n",
        "    print(f\"  Average TMIN: {weather_df['TMIN'].mean():.1f}Â°C\")\n",
        "    print(f\"  Average PRCP: {weather_df['PRCP'].mean():.1f}mm\")\n",
        "    print(f\"  Average wind: {weather_df['wind_mph'].mean():.1f} mph\")\n",
        "    print(f\"  Fire season days: {weather_df['fire_season'].sum()}\")\n",
        "    print(f\"  Max days since rain: {weather_df['days_since_rain'].max()}\")\n",
        "\n",
        "    # Wind strength summary\n",
        "    print(f\"\\nðŸŒªï¸ Wind strength summary:\")\n",
        "    print(weather_df['wind_strength'].value_counts())\n",
        "\n",
        "    return weather_df\n",
        "\n",
        "def add_wind_strength(weather_df):\n",
        "    \"\"\"\n",
        "    Add simple wind strength categories for fire danger.\n",
        "\n",
        "    Args:\n",
        "        weather_df: Weather dataframe with 'AWND' column\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with wind strength features added\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert wind speed to mph (AWND is in m/s)\n",
        "    weather_df['wind_mph'] = weather_df['AWND'] * 2.237\n",
        "\n",
        "    # Simple wind strength categories\n",
        "    def get_wind_strength(wind_mph):\n",
        "        if wind_mph >= 25:\n",
        "            return 'EXTREME'      # Santa Ana/Diablo conditions\n",
        "        elif wind_mph >= 15:\n",
        "            return 'HIGH'         # Fire danger threshold\n",
        "        elif wind_mph >= 8:\n",
        "            return 'MODERATE'     # Elevated risk\n",
        "        else:\n",
        "            return 'LOW'          # Normal conditions\n",
        "\n",
        "    weather_df['wind_strength'] = weather_df['wind_mph'].apply(get_wind_strength)\n",
        "\n",
        "    return weather_df\n",
        "\n",
        "# Load weather data\n",
        "print(\"Loading weather data...\")\n",
        "weather_data = load_weather_data()\n",
        "\n",
        "if weather_data is not None:\n",
        "    print(f\"\\nWeather data shape: {weather_data.shape}\")\n",
        "    print(\"\\nSample weather data:\")\n",
        "    print(weather_data.head())\n",
        "else:\n",
        "    print(\"Failed to load weather data.\")"
      ],
      "metadata": {
        "id": "SYIu4uVMVzNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Google Earth Engine Code for NDVI Data Retrieval**"
      ],
      "metadata": {
        "id": "qCN1M2N-IvCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ee\n",
        "\n",
        "# Initialize GEE\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project='fire-prediction-ee-salvatore')\n",
        "\n",
        "# Define California state boundary for intersection\n",
        "california = ee.FeatureCollection(\"TIGER/2018/States\") \\\n",
        "    .filter(ee.Filter.eq('STUSPS', 'CA')) \\\n",
        "    .geometry()\n",
        "\n",
        "# Updated comprehensive weather station locations\n",
        "weather_stations = [\n",
        "    {'name': 'Arcata Eureka Airport', 'usw': 'USW00024283', 'lat': 40.97844, 'lon': -124.10479},\n",
        "    {'name': 'Redding Airport', 'usw': 'USW00024257', 'lat': 40.51, 'lon': -122.29},\n",
        "    {'name': 'Marysville Airport (Beale AFB)', 'usw': 'USW00093205', 'lat': 39.136089, 'lon': -121.436567},\n",
        "    {'name': 'Napa Airport', 'usw': 'USW00093227', 'lat': 38.213194, 'lon': -122.280694},\n",
        "    {'name': 'Stockton Airport', 'usw': 'USW00023237', 'lat': 37.88997, 'lon': -121.22637},\n",
        "    {'name': 'Fresno Yosemite International', 'usw': 'USW00093193', 'lat': 36.77999, 'lon': -119.72016},\n",
        "    {'name': 'Santa Maria Airport', 'usw': 'USW00023273', 'lat': 34.8927, 'lon': -120.4545},\n",
        "    {'name': 'Watsonville Airport', 'usw': 'USW00023277', 'lat': 36.935, 'lon': -121.79},\n",
        "    {'name': 'Merced Municipal Airport', 'usw': 'USW00023257', 'lat': 37.28470, 'lon': -120.51400},\n",
        "    {'name': 'Bakersfield Airport', 'usw': 'USW00023155', 'lat': 35.3217, 'lon': -118.9910},\n",
        "    {'name': 'Santa Barbara 11W', 'usw': 'USW00023190', 'lat': 34.4208, 'lon': -119.6982},\n",
        "    {'name': 'Burbank-Glendale-Pasadena Airport', 'usw': 'USW00023152', 'lat': 34.2003, 'lon': -118.3552},\n",
        "    {'name': 'Oceanside Airport', 'usw': 'USW00023181', 'lat': 33.218, 'lon': -117.351}\n",
        "]\n",
        "\n",
        "# Square size in kilometers\n",
        "square_size_km = 80\n",
        "\n",
        "def create_weather_station_square(station_info, size_km):\n",
        "    \"\"\"Create a square polygon around a weather station\"\"\"\n",
        "    lat = station_info['lat']\n",
        "    lon = station_info['lon']\n",
        "\n",
        "    # Convert km to degrees (approximate: 1 degree â‰ˆ 111 km)\n",
        "    size_deg = size_km / 111\n",
        "\n",
        "    # Create square coordinates\n",
        "    coords = [\n",
        "        [lon - size_deg/2, lat - size_deg/2],  # southwest\n",
        "        [lon + size_deg/2, lat - size_deg/2],  # southeast\n",
        "        [lon + size_deg/2, lat + size_deg/2],  # northeast\n",
        "        [lon - size_deg/2, lat + size_deg/2],  # northwest\n",
        "        [lon - size_deg/2, lat - size_deg/2]   # close polygon\n",
        "    ]\n",
        "\n",
        "    polygon = ee.Geometry.Polygon([coords])\n",
        "\n",
        "    # Intersect with California boundary to ensure we stay within state\n",
        "    return polygon.intersection(california)\n",
        "\n",
        "# Create weather station regions dictionary\n",
        "weather_regions = {}\n",
        "for station in weather_stations:\n",
        "    # Use USW ID as region identifier (clean version for naming)\n",
        "    region_key = f\"{station['usw']}_{station['name'].replace(' ', '_').replace('(', '').replace(')', '').replace('-', '_')}\"\n",
        "    weather_regions[region_key] = create_weather_station_square(station, square_size_km)\n",
        "\n",
        "# Load MODIS NDVI\n",
        "modis = ee.ImageCollection('MODIS/061/MOD13Q1') \\\n",
        "    .select('NDVI') \\\n",
        "    .filterDate('2009-01-01', '2013-12-31') \\\n",
        "    .map(lambda img: img.multiply(0.0001).copyProperties(img, ['system:time_start']))\n",
        "\n",
        "# Extract NDVI for all weather station regions\n",
        "features = []\n",
        "for region_name, geometry in weather_regions.items():\n",
        "    def extract_ndvi(image):\n",
        "        stats = image.reduceRegion(\n",
        "            reducer=ee.Reducer.mean().combine(\n",
        "                ee.Reducer.stdDev().combine(\n",
        "                    ee.Reducer.min().combine(\n",
        "                        ee.Reducer.max(),\n",
        "                        sharedInputs=True\n",
        "                    ),\n",
        "                    sharedInputs=True\n",
        "                ),\n",
        "                sharedInputs=True\n",
        "            ),\n",
        "            geometry=geometry,\n",
        "            scale=250,\n",
        "            maxPixels=1e9\n",
        "        )\n",
        "\n",
        "        return ee.Feature(None, {\n",
        "            'region': region_name,\n",
        "            'usw_id': region_name.split('_')[0],  # Extract USW ID\n",
        "            'station_name': '_'.join(region_name.split('_')[1:]),  # Extract station name\n",
        "            'date': ee.Date(image.get('system:time_start')).format('YYYY-MM-dd'),\n",
        "            'ndvi_mean': stats.get('NDVI_mean'),\n",
        "            'ndvi_stddev': stats.get('NDVI_stdDev'),\n",
        "            'ndvi_min': stats.get('NDVI_min'),\n",
        "            'ndvi_max': stats.get('NDVI_max')\n",
        "        })\n",
        "\n",
        "    region_features = modis.map(extract_ndvi)\n",
        "    features.append(region_features)\n",
        "\n",
        "# Combine and export\n",
        "combined = ee.FeatureCollection(features).flatten()\n",
        "\n",
        "# Export task\n",
        "task = ee.batch.Export.table.toDrive(\n",
        "    collection=combined,\n",
        "    description='california_weather_station_ndvi',\n",
        "    folder='fire_prediction_project/raw_data/vegetation_modis',\n",
        "    fileNamePrefix='california_weather_station_ndvi',\n",
        "    fileFormat='CSV'\n",
        ")\n",
        "\n",
        "task.start()\n",
        "print(\"Export task started!\")\n",
        "print(\"Monitor progress at: https://code.earthengine.google.com/tasks\")\n",
        "\n",
        "# Print summary information\n",
        "print(f\"\\nProcessing {len(weather_stations)} weather stations:\")\n",
        "for station in weather_stations:\n",
        "    print(f\"  - {station['name']} ({station['usw']})\")\n",
        "print(f\"\\nSquare size: {square_size_km}km x {square_size_km}km\")\n",
        "print(f\"Total regions: {len(weather_regions)}\")\n"
      ],
      "metadata": {
        "id": "dUqMZW19zI3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_and_clean_ndvi():\n",
        "    \"\"\"Load, clean, and prepare NDVI data\"\"\"\n",
        "    # Load NDVI data\n",
        "    ndvi_path = '/content/drive/MyDrive/fire_prediction_project/raw_data/vegetation_modis/california_weather_station_ndvi.csv'\n",
        "    ndvi_df = pd.read_csv(ndvi_path)\n",
        "\n",
        "    # Clean data\n",
        "    ndvi_df['date'] = pd.to_datetime(ndvi_df['date'])\n",
        "    ndvi_df = ndvi_df.dropna(subset=['ndvi_mean'])\n",
        "    ndvi_df = ndvi_df[(ndvi_df['ndvi_mean'] >= -0.2) & (ndvi_df['ndvi_mean'] <= 1.0)]\n",
        "\n",
        "    # Add basic features\n",
        "    ndvi_df['vegetation_stress'] = 1 - ndvi_df['ndvi_mean'].clip(0, 1)\n",
        "    ndvi_df['month'] = ndvi_df['date'].dt.month\n",
        "    ndvi_df['day_of_year'] = ndvi_df['date'].dt.dayofyear\n",
        "\n",
        "    print(f\"Shape: {ndvi_df.shape}\")\n",
        "    print(f\"Date range: {ndvi_df['date'].min().date()} to {ndvi_df['date'].max().date()}\")\n",
        "    print(f\"Regions: {ndvi_df['region'].nunique()}\")\n",
        "\n",
        "    return ndvi_df\n",
        "\n",
        "# Load NDVI data\n",
        "ndvi_data = load_and_clean_ndvi()\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(ndvi_data.head())"
      ],
      "metadata": {
        "id": "1A7a388_ROQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"Set2\")\n",
        "\n",
        "def visualize_ndvi(ndvi_data):\n",
        "    \"\"\"Simple NDVI visualizations\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('NDVI Data Overview', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # 1. NDVI by Region (Box plot)\n",
        "    axes[0,0].boxplot([ndvi_data[ndvi_data['region']==r]['ndvi_mean']\n",
        "                       for r in ndvi_data['region'].unique()],\n",
        "                      labels=ndvi_data['region'].unique())\n",
        "    axes[0,0].set_title('NDVI Distribution by Region')\n",
        "    axes[0,0].set_ylabel('NDVI')\n",
        "    axes[0,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # 2. NDVI Time Series by Region\n",
        "    for region in ndvi_data['region'].unique():\n",
        "        region_data = ndvi_data[ndvi_data['region'] == region]\n",
        "        axes[0,1].plot(region_data['date'], region_data['ndvi_mean'],\n",
        "                       label=region, alpha=0.7)\n",
        "    axes[0,1].set_title('NDVI Over Time')\n",
        "    axes[0,1].set_ylabel('NDVI')\n",
        "    axes[0,1].legend()\n",
        "    axes[0,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # 3. Seasonal NDVI Pattern\n",
        "    monthly_ndvi = ndvi_data.groupby('month')['ndvi_mean'].mean()\n",
        "    axes[1,0].plot(monthly_ndvi.index, monthly_ndvi.values, 'o-', linewidth=2)\n",
        "    axes[1,0].set_title('Seasonal NDVI Pattern')\n",
        "    axes[1,0].set_xlabel('Month')\n",
        "    axes[1,0].set_ylabel('Average NDVI')\n",
        "    axes[1,0].set_xticks(range(1,13))\n",
        "\n",
        "    # 4. Vegetation Stress Distribution\n",
        "    axes[1,1].hist(ndvi_data['vegetation_stress'], bins=30, alpha=0.7, edgecolor='black')\n",
        "    axes[1,1].set_title('Vegetation Stress Distribution')\n",
        "    axes[1,1].set_xlabel('Vegetation Stress (1 - NDVI)')\n",
        "    axes[1,1].set_ylabel('Frequency')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Summary stats\n",
        "    print(\"\\nðŸ“Š NDVI Summary by Region:\")\n",
        "    summary = ndvi_data.groupby('region')['ndvi_mean'].agg(['mean', 'std', 'min', 'max'])\n",
        "    print(summary.round(3))\n",
        "\n",
        "# Run visualization\n",
        "visualize_ndvi(ndvi_data)"
      ],
      "metadata": {
        "id": "CP6GnXvKSVgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**California Wildfire Risk Prediction LSTM Model**"
      ],
      "metadata": {
        "id": "Fc_7XcGsD8nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Imports**"
      ],
      "metadata": {
        "id": "T00OzqtXEPkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# scikit-learn imports for categorical encoding and preprocessing\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Visualization imports\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# System and warning management\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "LXtTfGrPEJJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Merging all of the Data Sets**"
      ],
      "metadata": {
        "id": "GqNxTOTcEgXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_and_merge_data():\n",
        "    \"\"\"\n",
        "    Collect and merge fire, weather, and NDVI data into training dataset\n",
        "    \"\"\"\n",
        "    print(\"Loading and merging datasets...\")\n",
        "\n",
        "    try:\n",
        "        # Fire targets data\n",
        "        fire_targets_df = fire_targets.copy()\n",
        "\n",
        "        # Weather data\n",
        "        weather_df = weather_data.copy()\n",
        "\n",
        "        # NDVI data\n",
        "        ndvi_df = ndvi_data.copy()\n",
        "\n",
        "        print(f\"Fire targets: {len(fire_targets_df)} records\")\n",
        "        print(f\"Weather data: {len(weather_df)} records\")\n",
        "        print(f\"NDVI data: {len(ndvi_df)} records\")\n",
        "\n",
        "    except NameError:\n",
        "        print(\"Error: Please run the data loading cells first!\")\n",
        "        return None\n",
        "\n",
        "    # Merge datasets\n",
        "    print(\"Merging datasets...\")\n",
        "\n",
        "    # Merge fire targets with weather data\n",
        "    merged_df = fire_targets_df.merge(\n",
        "        weather_df,\n",
        "        on=['date', 'weather_station'],\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # For NDVI data, we need to map weather stations to regions\n",
        "    # Create a mapping from weather station to region (usw_id)\n",
        "    station_mapping = {\n",
        "        'USW00024283': 'USW00024283_Arcata_Eureka_Airport',\n",
        "        'USW00024257': 'USW00024257_Redding_Airport',\n",
        "        'USW00093205': 'USW00093205_Marysville_Airport_Beale_AFB',\n",
        "        'USW00093227': 'USW00093227_Napa_Airport',\n",
        "        'USW00023237': 'USW00023237_Stockton_Airport',\n",
        "        'USW00093193': 'USW00093193_Fresno_Yosemite_International',\n",
        "        'USW00023273': 'USW00023273_Santa_Maria_Airport',\n",
        "        'USW00023277': 'USW00023277_Watsonville_Airport',\n",
        "        'USW00023257': 'USW00023257_Merced_Municipal_Airport',\n",
        "        'USW00023155': 'USW00023155_Bakersfield_Airport',\n",
        "        'USW00023190': 'USW00023190_Santa_Barbara_11W',\n",
        "        'USW00023152': 'USW00023152_Burbank_Glendale_Pasadena_Airport',\n",
        "        'USW00023181': 'USW00023181_Oceanside_Airport'\n",
        "    }\n",
        "\n",
        "    # Add region mapping to merged data\n",
        "    merged_df['region'] = merged_df['weather_station'].map(station_mapping)\n",
        "\n",
        "    # Merge with NDVI data\n",
        "    final_df = merged_df.merge(\n",
        "        ndvi_df[['date', 'region', 'ndvi_mean', 'vegetation_stress']],\n",
        "        on=['date', 'region'],\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    print(f\"Final merged dataset: {len(final_df)} records\")\n",
        "\n",
        "    # Handle missing values\n",
        "    initial_count = len(final_df)\n",
        "    final_df = final_df.dropna()\n",
        "\n",
        "    return final_df"
      ],
      "metadata": {
        "id": "BSfctiu9EhXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Engineering**"
      ],
      "metadata": {
        "id": "fDu84XC3KYeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_rolling_features(df, windows=[3, 7]):\n",
        "    \"\"\"\n",
        "    Create rolling window features for key weather variables per station:\n",
        "      - mean, std, max over specified lookback windows\n",
        "    \"\"\"\n",
        "    df = df.sort_values(['weather_station', 'date'])\n",
        "    weather_vars = ['TMAX', 'TMIN', 'PRCP', 'AWND', 'temperature_range']\n",
        "    existing = [v for v in weather_vars if v in df.columns]\n",
        "\n",
        "    for var in existing:\n",
        "        grp = df.groupby('weather_station')[var]\n",
        "        for w in windows:\n",
        "            col_mean = f'{var}_rolling_mean_{w}d'\n",
        "            col_std  = f'{var}_rolling_std_{w}d'\n",
        "            col_max  = f'{var}_rolling_max_{w}d'\n",
        "            df[col_mean] = grp.transform(lambda x: x.rolling(window=w, min_periods=1).mean())\n",
        "            df[col_std]  = grp.transform(lambda x: x.rolling(window=w, min_periods=1).std())\n",
        "            df[col_max]  = grp.transform(lambda x: x.rolling(window=w, min_periods=1).max())\n",
        "\n",
        "    return df\n",
        "\n",
        "def prepare_features(df):\n",
        "    \"\"\"\n",
        "    Prepare and scale features for LSTM training:\n",
        "      1) Original weather & engineered features\n",
        "      2) Temporal features\n",
        "      3) Station encoding\n",
        "      4) Rolling window features\n",
        "    Returns: X_scaled, y, dates, stations, scaler, feature_columns\n",
        "    \"\"\"\n",
        "    print(\"\\n Engineering features...\")\n",
        "    df = df.sort_values(['weather_station', 'date']).copy()\n",
        "\n",
        "    # Original & engineered\n",
        "    original = ['PRCP', 'TMAX', 'TMIN', 'AWND',\n",
        "                'temperature_range', 'days_since_rain',\n",
        "                'fire_season', 'ndvi_mean', 'vegetation_stress']\n",
        "\n",
        "    # Temporal\n",
        "    df['month']        = df['date'].dt.month\n",
        "    df['day_of_year']  = df['date'].dt.dayofyear\n",
        "    df['week_of_year'] = df['date'].dt.isocalendar().week\n",
        "    temporal = ['month', 'day_of_year', 'week_of_year']\n",
        "\n",
        "    # Station encoding\n",
        "    le = LabelEncoder()\n",
        "    df['station_encoded'] = le.fit_transform(df['weather_station'])\n",
        "    station = ['station_encoded']\n",
        "\n",
        "    # Rolling\n",
        "    df = create_rolling_features(df)\n",
        "    rolling = [c for c in df.columns if '_rolling_' in c]\n",
        "\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # Assemble features\n",
        "    feature_columns = original + temporal + station + rolling\n",
        "    print(f\"Total features: {len(feature_columns)}\")\n",
        "\n",
        "    X = df[feature_columns].values\n",
        "    y = df['fire_within_5_days'].values\n",
        "    dates = df['date'].values\n",
        "    stations = df['weather_station'].values\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    print(f\"âœ… Scaled features\")\n",
        "    print(f\"ðŸ“Š Target distribution: {np.bincount(y)}\")\n",
        "\n",
        "    return X_scaled, y, dates, stations, scaler, feature_columns"
      ],
      "metadata": {
        "id": "0qBML6DGKpJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sequence for LSTM**"
      ],
      "metadata": {
        "id": "7pVfNt-UN7pS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(X, y, dates, stations, sequence_length=7):\n",
        "    \"\"\"\n",
        "    Create sequences for LSTM training while maintaining temporal order\n",
        "    \"\"\"\n",
        "    print(f\"\\n Creating sequences with length {sequence_length}...\")\n",
        "\n",
        "    sequences_X = []\n",
        "    sequences_y = []\n",
        "    sequence_dates = []\n",
        "    sequence_stations = []\n",
        "\n",
        "    # Create a DataFrame for easier sorting and grouping\n",
        "    df_temp = pd.DataFrame({\n",
        "        'station': stations,\n",
        "        'date': pd.to_datetime(dates),\n",
        "        'target': y\n",
        "    })\n",
        "\n",
        "    # Add feature columns individually to avoid the assignment error\n",
        "    for i in range(X.shape[1]):\n",
        "        df_temp[f'feature_{i}'] = X[:, i]\n",
        "\n",
        "    # Create sequences for each station separately\n",
        "    for station in df_temp['station'].unique():\n",
        "        station_data = df_temp[df_temp['station'] == station].copy()\n",
        "\n",
        "        # Sort by date to ensure temporal order\n",
        "        station_data = station_data.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "        # Extract features back to array format\n",
        "        feature_cols = [col for col in station_data.columns if col.startswith('feature_')]\n",
        "        station_X = station_data[feature_cols].values\n",
        "        station_y = station_data['target'].values\n",
        "        station_dates = station_data['date'].values\n",
        "\n",
        "        # Create sequences for this station\n",
        "        for i in range(len(station_X) - sequence_length + 1):\n",
        "            sequences_X.append(station_X[i:(i + sequence_length)])\n",
        "            sequences_y.append(station_y[i + sequence_length - 1])  # Predict last day of sequence\n",
        "            sequence_dates.append(station_dates[i + sequence_length - 1])\n",
        "            sequence_stations.append(station)\n",
        "\n",
        "    sequences_X = np.array(sequences_X)\n",
        "    sequences_y = np.array(sequences_y)\n",
        "\n",
        "    print(f\"Created {len(sequences_X)} sequences\")\n",
        "    print(f\"Sequence shape: {sequences_X.shape}\")\n",
        "    print(f\"Target distribution in sequences: {np.bincount(sequences_y)}\")\n",
        "\n",
        "    return sequences_X, sequences_y, np.array(sequence_dates), np.array(sequence_stations)"
      ],
      "metadata": {
        "id": "olZauyvnOAUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Time-based Train/Validation Split**"
      ],
      "metadata": {
        "id": "EN-SQZ49OW6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def time_based_split(X, y, dates, test_size=0.2):\n",
        "    \"\"\"\n",
        "    Split data based on time to maintain temporal integrity\n",
        "    \"\"\"\n",
        "    print(f\"\\nCreating time-based train/validation split...\")\n",
        "\n",
        "    # Sort by date\n",
        "    sort_indices = np.argsort(dates)\n",
        "    X_sorted = X[sort_indices]\n",
        "    y_sorted = y[sort_indices]\n",
        "    dates_sorted = dates[sort_indices]\n",
        "\n",
        "    # Split based on time\n",
        "    split_idx = int(len(X_sorted) * (1 - test_size))\n",
        "\n",
        "    X_train = X_sorted[:split_idx]\n",
        "    X_val = X_sorted[split_idx:]\n",
        "    y_train = y_sorted[:split_idx]\n",
        "    y_val = y_sorted[split_idx:]\n",
        "\n",
        "    train_end_date = dates_sorted[split_idx-1]\n",
        "    val_start_date = dates_sorted[split_idx]\n",
        "\n",
        "    print(f\"Training data: {len(X_train)} sequences (up to {train_end_date})\")\n",
        "    print(f\"Validation data: {len(X_val)} sequences (from {val_start_date})\")\n",
        "    print(f\"Training target distribution: {np.bincount(y_train)}\")\n",
        "    print(f\"Validation target distribution: {np.bincount(y_val)}\")\n",
        "\n",
        "    return X_train, X_val, y_train, y_val"
      ],
      "metadata": {
        "id": "MruZw7tJOWBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Overfitting Prevention**"
      ],
      "metadata": {
        "id": "prrOpsu5Ow7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_model(input_shape, learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Build LSTM model with comprehensive overfitting prevention\n",
        "    Research-based approach using proven regularization techniques\n",
        "    \"\"\"\n",
        "    print(f\"\\n Building LSTM model with input shape {input_shape}...\")\n",
        "\n",
        "    model = Sequential([\n",
        "        # Input dropout to prevent overfitting on input features\n",
        "        Dropout(0.1, input_shape=input_shape, name='input_dropout'),\n",
        "\n",
        "        # First LSTM layer - moderate size to prevent overfitting\n",
        "        LSTM(64,\n",
        "             return_sequences=True,\n",
        "             kernel_regularizer=l2(0.01),  # L2 regularization on weights\n",
        "             recurrent_regularizer=l2(0.01),  # L2 on recurrent weights\n",
        "             name='lstm_1'),\n",
        "\n",
        "        # Batch normalization for training stability\n",
        "        BatchNormalization(name='batch_norm_1'),\n",
        "\n",
        "        # Dropout after first LSTM\n",
        "        Dropout(0.3, name='dropout_1'),\n",
        "\n",
        "        # Second LSTM layer - smaller to create funnel effect\n",
        "        LSTM(32,\n",
        "             kernel_regularizer=l2(0.01),\n",
        "             recurrent_regularizer=l2(0.01),\n",
        "             name='lstm_2'),\n",
        "\n",
        "        # Batch normalization\n",
        "        BatchNormalization(name='batch_norm_2'),\n",
        "\n",
        "        # Dropout before dense layer\n",
        "        Dropout(0.4, name='dropout_2'),\n",
        "\n",
        "        # Dense layer with regularization\n",
        "        Dense(16,\n",
        "              activation='relu',\n",
        "              kernel_regularizer=l2(0.01),\n",
        "              name='dense_1'),\n",
        "\n",
        "        # Final dropout\n",
        "        Dropout(0.2, name='dropout_final'),\n",
        "\n",
        "        # Output layer for binary classification\n",
        "        Dense(1, activation='sigmoid', name='output')\n",
        "    ])\n",
        "\n",
        "    # Compile with Adam optimizer and learning rate scheduling\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "        learning_rate=learning_rate,\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.999,\n",
        "        epsilon=1e-07\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', 'precision', 'recall']\n",
        "    )\n",
        "\n",
        "    print(\"âœ… Model compiled successfully\")\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "PDR0mxJ9OwUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Callback Overfitting prevention**"
      ],
      "metadata": {
        "id": "bqWns2dSPLqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_callbacks():\n",
        "    \"\"\"\n",
        "    Setup callbacks for comprehensive overfitting prevention\n",
        "    \"\"\"\n",
        "    print(\"\\nSetting up training callbacks...\")\n",
        "\n",
        "    callbacks = [\n",
        "        # Early stopping - stops training when validation loss stops improving\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,  # Wait 10 epochs before stopping\n",
        "            restore_best_weights=True,  # Restore best model weights\n",
        "            verbose=1,\n",
        "            mode='min'\n",
        "        ),\n",
        "\n",
        "        # Reduce learning rate when plateau is reached\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,  # Reduce LR by half\n",
        "            patience=5,   # Wait 5 epochs before reducing\n",
        "            min_lr=1e-7,  # Minimum learning rate\n",
        "            verbose=1,\n",
        "            mode='min'\n",
        "        ),\n",
        "\n",
        "        # Save best model\n",
        "        ModelCheckpoint(\n",
        "            '/content/drive/MyDrive/fire_prediction_project/best_wildfire_model.h5',\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            verbose=1,\n",
        "            mode='min'\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    print(\"âœ… Callbacks configured:\")\n",
        "    print(\"  - Early stopping with patience=10\")\n",
        "    print(\"  - Learning rate reduction on plateau\")\n",
        "    print(\"  - Model checkpointing\")\n",
        "\n",
        "    return callbacks"
      ],
      "metadata": {
        "id": "7co5A1R5PbLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Model Training**"
      ],
      "metadata": {
        "id": "CWUXMH2BPkFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, X_train, y_train, X_val, y_val, callbacks, epochs=100, batch_size=32):\n",
        "    \"\"\"\n",
        "    Train the LSTM model with proper monitoring\n",
        "    \"\"\"\n",
        "    print(f\"\\n Training model for {epochs} epochs...\")\n",
        "    print(f\" Batch size: {batch_size}\")\n",
        "\n",
        "    # Class weights to handle imbalanced data\n",
        "    unique, counts = np.unique(y_train, return_counts=True)\n",
        "    class_weight = {0: 1.0, 1: 15.0}\n",
        "\n",
        "    print(f\"âš–ï¸ Class weights: {class_weight}\")\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=callbacks,\n",
        "        class_weight=class_weight,\n",
        "        verbose=1,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    print(\"âœ… Training completed!\")\n",
        "    return history"
      ],
      "metadata": {
        "id": "DgOyOidsPp-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Evaluation**"
      ],
      "metadata": {
        "id": "tA1oYxzhP2Ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_optimal_threshold(model, X_val, y_val):\n",
        "    \"\"\"Find threshold that maximizes F2 score (emphasizes recall for fire detection)\"\"\"\n",
        "    y_pred_prob = model.predict(X_val).flatten()\n",
        "\n",
        "    thresholds = np.arange(0.05, 0.95, 0.01)  # Test many thresholds\n",
        "    best_threshold = 0.5\n",
        "    best_f2 = 0\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        y_pred = (y_pred_prob >= threshold).astype(int)\n",
        "\n",
        "        # F2 score emphasizes recall (finding fires)\n",
        "        precision = precision_score(y_val, y_pred, zero_division=0)\n",
        "        recall = recall_score(y_val, y_pred, zero_division=0)\n",
        "\n",
        "        if precision + recall > 0:\n",
        "            f2 = (5 * precision * recall) / (4 * precision + recall)\n",
        "            if f2 > best_f2:\n",
        "                best_f2 = f2\n",
        "                best_threshold = threshold\n",
        "\n",
        "    return best_threshold\n",
        "\n",
        "def evaluate_model(model, X_val, y_val):\n",
        "    \"\"\"\n",
        "    Comprehensive model evaluation\n",
        "    \"\"\"\n",
        "    print(\"\\n Evaluating model performance...\")\n",
        "\n",
        "    # Make predictions\n",
        "    optimal_threshold = find_optimal_threshold(model, X_val, y_val)\n",
        "    y_pred_prob = model.predict(X_val)\n",
        "    y_pred = (y_pred_prob > 0.25).astype(int).flatten()\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = np.mean(y_pred == y_val)\n",
        "    auc_score = roc_auc_score(y_val, y_pred_prob)\n",
        "\n",
        "    print(f\"ðŸŽ¯ Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"ðŸ“ˆ AUC Score: {auc_score:.4f}\")\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\nðŸ“‹ Classification Report:\")\n",
        "    print(classification_report(y_val, y_pred, target_names=['No Fire', 'Fire']))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    print(\"\\nðŸ” Confusion Matrix:\")\n",
        "    cm = confusion_matrix(y_val, y_pred)\n",
        "    print(cm)\n",
        "\n",
        "    return y_pred_prob, y_pred"
      ],
      "metadata": {
        "id": "Z92YqRcvP-3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training Model Visualization**"
      ],
      "metadata": {
        "id": "NUE9JDM0QQLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    Plot training history to check for overfitting\n",
        "    \"\"\"\n",
        "    print(\"\\nPlotting training history...\")\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('LSTM Training History - Overfitting Analysis', fontsize=16)\n",
        "\n",
        "    # Loss plot\n",
        "    axes[0,0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "    axes[0,0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "    axes[0,0].set_title('Model Loss')\n",
        "    axes[0,0].set_xlabel('Epoch')\n",
        "    axes[0,0].set_ylabel('Loss')\n",
        "    axes[0,0].legend()\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Accuracy plot\n",
        "    axes[0,1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
        "    axes[0,1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
        "    axes[0,1].set_title('Model Accuracy')\n",
        "    axes[0,1].set_xlabel('Epoch')\n",
        "    axes[0,1].set_ylabel('Accuracy')\n",
        "    axes[0,1].legend()\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Learning rate (if available)\n",
        "    if 'lr' in history.history:\n",
        "        axes[1,0].plot(history.history['lr'], linewidth=2, color='red')\n",
        "        axes[1,0].set_title('Learning Rate Schedule')\n",
        "        axes[1,0].set_xlabel('Epoch')\n",
        "        axes[1,0].set_ylabel('Learning Rate')\n",
        "        axes[1,0].set_yscale('log')\n",
        "        axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Overfitting analysis\n",
        "    train_loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    overfitting_gap = np.array(val_loss) - np.array(train_loss)\n",
        "\n",
        "    axes[1,1].plot(overfitting_gap, linewidth=2, color='orange')\n",
        "    axes[1,1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "    axes[1,1].set_title('Overfitting Gap (Val Loss - Train Loss)')\n",
        "    axes[1,1].set_xlabel('Epoch')\n",
        "    axes[1,1].set_ylabel('Loss Difference')\n",
        "    axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Overfitting analysis\n",
        "    final_gap = overfitting_gap[-5:].mean()  # Average of last 5 epochs\n",
        "    if final_gap > 0.1:\n",
        "        print(\" WARNING: Model may be overfitting (validation loss > training loss)\")\n",
        "    elif final_gap < -0.05:\n",
        "        print(\"WARNING: Model may be underfitting (training loss > validation loss)\")\n",
        "    else:\n",
        "        print(\"Good fit: Training and validation losses are well balanced\")\n"
      ],
      "metadata": {
        "id": "T2Pns1gZQajY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Main Function**"
      ],
      "metadata": {
        "id": "HEmj0PPXQlbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution function\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Collect and merge data\n",
        "    merged_data = collect_and_merge_data()\n",
        "    if merged_data is None:\n",
        "        return\n",
        "\n",
        "    # Step 2: Prepare features\n",
        "    X, y, dates, stations, scaler, feature_names = prepare_features(merged_data)\n",
        "\n",
        "    # Step 3: Create sequences\n",
        "    X_seq, y_seq, seq_dates, seq_stations = create_sequences(X, y, dates, stations, sequence_length=7)\n",
        "\n",
        "    # Step 4: Time-based split\n",
        "    X_train, X_val, y_train, y_val = time_based_split(X_seq, y_seq, seq_dates)\n",
        "\n",
        "    # Step 5: Build model\n",
        "    model = build_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
        "\n",
        "    # Step 6: Setup callbacks\n",
        "    callbacks = setup_callbacks()\n",
        "\n",
        "    # Step 7: Train model\n",
        "    history = train_model(model, X_train, y_train, X_val, y_val, callbacks, epochs=100, batch_size=32)\n",
        "\n",
        "    # Step 8: Evaluate model\n",
        "    y_pred_prob, y_pred = evaluate_model(model, X_val, y_val)\n",
        "\n",
        "    # Step 9: Plot results\n",
        "    plot_training_history(history)\n",
        "\n",
        "    print(\"\\nðŸŽ‰ LSTM training pipeline completed successfully!\")\n",
        "    print(\"ðŸ“ Best model saved to: /content/drive/MyDrive/fire_prediction_project/best_wildfire_model.h5\")\n",
        "\n",
        "    return model, history, scaler, feature_names\n",
        "\n",
        "# Run the complete pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    model, history, scaler, feature_names = main()"
      ],
      "metadata": {
        "id": "zfV8y_NYQr7L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}